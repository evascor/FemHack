{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_idx</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>target</th>\n",
       "      <th>creator_name</th>\n",
       "      <th>creator_location</th>\n",
       "      <th>supporters</th>\n",
       "      <th>created_date</th>\n",
       "      <th>banner_image</th>\n",
       "      <th>full_content</th>\n",
       "      <th>victory_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Stop Iran Regime from Chairing U​.​N. Human Ri...</td>\n",
       "      <td>Rights Council 2023 Social Forum. This year's ...</td>\n",
       "      <td>['António Guterres']</td>\n",
       "      <td>UN Watch</td>\n",
       "      <td>Geneva, Switzerland</td>\n",
       "      <td>80515</td>\n",
       "      <td>10 May 2023</td>\n",
       "      <td>https://assets.change.org/photos/2/ur/wj/vrUrW...</td>\n",
       "      <td>No Joke: The Islamic Republic of Iran has just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Save Children In Poverty In Syria</td>\n",
       "      <td>Children in Syria are facing extreme poverty. ...</td>\n",
       "      <td>['UNICEF']</td>\n",
       "      <td>Maialen Alawam</td>\n",
       "      <td>Watertown, MA, United States</td>\n",
       "      <td>23871</td>\n",
       "      <td>25 Jun 2020</td>\n",
       "      <td>https:https://static.change.org/images/default...</td>\n",
       "      <td>Children in Syria are facing extreme poverty. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Save Duke Ellington School Of The Arts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It’s Me Tivoni</td>\n",
       "      <td>United States</td>\n",
       "      <td>4051</td>\n",
       "      <td>Nov. 20, 2022</td>\n",
       "      <td>https://assets.change.org/photos/8/ab/hv/DZaBH...</td>\n",
       "      <td>Save Duke Ellington School Of The ArtsTivoni H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Mobile operators are creating inequality</td>\n",
       "      <td>In India, after Reliance Jio (a 4G only mobile...</td>\n",
       "      <td>['Telecom Regulatory Authority of India', 'CEL...</td>\n",
       "      <td>Swastik Raj Chauhan</td>\n",
       "      <td>Ghaziabad, India</td>\n",
       "      <td>350</td>\n",
       "      <td>Oct 15, 2017</td>\n",
       "      <td>https://assets.change.org/photos/7/by/in/Kzbyi...</td>\n",
       "      <td>In India, after Reliance Jio (a 4G only mobile...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Grant Military Burial Honors to Women WWII Pilots</td>\n",
       "      <td>During World War II, a brave group of women jo...</td>\n",
       "      <td>['U.S. Senate']</td>\n",
       "      <td>Tiffany Miller @tiffbmiller</td>\n",
       "      <td>Walnut Creek, CA, United States</td>\n",
       "      <td>176092</td>\n",
       "      <td>2 Dec 2015</td>\n",
       "      <td>https://assets.change.org/photos/3/lh/jc/TmLhJ...</td>\n",
       "      <td>During World War II, a brave group of women jo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_idx                                              title  \\\n",
       "0          0  Stop Iran Regime from Chairing U​.​N. Human Ri...   \n",
       "1          1                  Save Children In Poverty In Syria   \n",
       "2          2             Save Duke Ellington School Of The Arts   \n",
       "3          3           Mobile operators are creating inequality   \n",
       "4          4  Grant Military Burial Honors to Women WWII Pilots   \n",
       "\n",
       "                                         description  \\\n",
       "0  Rights Council 2023 Social Forum. This year's ...   \n",
       "1  Children in Syria are facing extreme poverty. ...   \n",
       "2                                                NaN   \n",
       "3  In India, after Reliance Jio (a 4G only mobile...   \n",
       "4  During World War II, a brave group of women jo...   \n",
       "\n",
       "                                              target  \\\n",
       "0                               ['António Guterres']   \n",
       "1                                         ['UNICEF']   \n",
       "2                                                NaN   \n",
       "3  ['Telecom Regulatory Authority of India', 'CEL...   \n",
       "4                                    ['U.S. Senate']   \n",
       "\n",
       "                  creator_name                 creator_location  supporters  \\\n",
       "0                     UN Watch              Geneva, Switzerland       80515   \n",
       "1               Maialen Alawam     Watertown, MA, United States       23871   \n",
       "2               It’s Me Tivoni                    United States        4051   \n",
       "3          Swastik Raj Chauhan                 Ghaziabad, India         350   \n",
       "4  Tiffany Miller @tiffbmiller  Walnut Creek, CA, United States      176092   \n",
       "\n",
       "    created_date                                       banner_image  \\\n",
       "0    10 May 2023  https://assets.change.org/photos/2/ur/wj/vrUrW...   \n",
       "1    25 Jun 2020  https:https://static.change.org/images/default...   \n",
       "2  Nov. 20, 2022  https://assets.change.org/photos/8/ab/hv/DZaBH...   \n",
       "3   Oct 15, 2017  https://assets.change.org/photos/7/by/in/Kzbyi...   \n",
       "4     2 Dec 2015  https://assets.change.org/photos/3/lh/jc/TmLhJ...   \n",
       "\n",
       "                                        full_content  victory_flag  \n",
       "0  No Joke: The Islamic Republic of Iran has just...             0  \n",
       "1  Children in Syria are facing extreme poverty. ...             0  \n",
       "2  Save Duke Ellington School Of The ArtsTivoni H...             0  \n",
       "3  In India, after Reliance Jio (a 4G only mobile...             0  \n",
       "4  During World War II, a brave group of women jo...             1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobación existencia valores nulos\n",
    "df.isna().sum()\n",
    "# Limpiar valores nulos:\n",
    "df_purged=df.dropna()\n",
    "df_purged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos si existen valores duplicados\n",
    "df_purged.duplicated().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Crear una copia explícita del DataFrame\n",
    "df_purged = df_purged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar data no relevante \n",
    "    #  Columna con la url de los banners\n",
    "df_purged.drop('banner_image', axis=1, inplace=True)\n",
    "    #  Columna nombre del creador\n",
    "df_purged.drop('creator_name', axis=1, inplace=True)\n",
    "    #  Columna fecha de creacion\n",
    "df_purged.drop('created_date', axis=1, inplace=True) \n",
    "df_purged\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECCION IDIOMAS PETICIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "%pip install langdetect\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from langdetect import detect\n",
    "\n",
    "def detect_language(text):\n",
    "    # Tokenización de palabras\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Lematización de palabras\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Detección del idioma\n",
    "    language = detect(' '.join(tokens))\n",
    "    return language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna donde guardamos el idioma de la peticion\n",
    "df_purged['language'] = df_purged['title'].apply(detect_language)\n",
    "df_purged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de idiomas de las peticiones\n",
    "df_purged['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerar los 5 idiomas con mayores peticiones\n",
    "df_purged['language'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los 5 idiomas más usados\n",
    "top_languages = df_purged['language'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Filtrar el DataFrame para incluir solo las peticiones en los 5 idiomas más utilizados\n",
    "df_filtered = df_purged[df_purged['language'].isin(top_languages)]\n",
    "\n",
    "# Imprimir el DataFrame filtrado\n",
    "(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobación solidez de datos y consistencia\n",
    "# Extraer solo el país de la columna 'creator_location'\n",
    "df_filtered['creator_location'] = df_filtered['creator_location'].str.split(',').str[-1].str.strip()\n",
    "df_filtered['creator_location'].value_counts()\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRADUCIR AL INGLÉS LAS PETICIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "import pandas as pd\n",
    "\n",
    "# Definir una función para traducir una columna\n",
    "def translate_column(column):\n",
    "    translated_column = []\n",
    "    for text in column:\n",
    "        translation = translator.translate(text,dest='en').text\n",
    "        translated_column.append(translation)\n",
    "    return translated_column\n",
    "# \n",
    "# Traducir la columna 'title' y almacenar los resultados en una nueva columna\n",
    "df_filtered[\"title_translated\"] = translate_column(df_filtered[\"title\"])\n",
    "\n",
    "# Traducir la columna 'description' y almacenar los resultados en una nueva columna\n",
    "df_filtered[\"description_translated\"] = translate_column(df_filtered[\"description\"])\n",
    "\n",
    "# Traducir la columna 'full_content' y almacenar los resultados en una nueva columna\n",
    "df_filtered[\"full_content_translated\"] = translate_column(df_filtered[\"full_content\"])\n",
    "\n",
    "# Mostrar el dataframe resultante\n",
    "df_filtered\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELIMINAR PALABRAS IRRELEVANTES (STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar los stopwords si aún no están descargados\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Obtener los stopwords en el idioma deseado (en este caso, inglés)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Definir una función para eliminar las palabras irrelevantes de un texto\n",
    "def remove_stopwords(text):\n",
    "    tokens = word_tokenize(text.upper())\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "# Aplicar la función remove_stopwords a la columna 'title_translated' del dataframe df_filtered\n",
    "df_filtered['title_cleaned_nostopwords'] = df_filtered['title_translated'].apply(remove_stopwords)\n",
    "df_filtered['description_nostopwords'] = df_filtered['description_translated'].apply(remove_stopwords)\n",
    "df_filtered['full_content_nostopwords'] = df_filtered['full_content_translated'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "# Mostrar el dataframe resultante\n",
    "df_filtered\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTAR LAS PALABRAS MÁS REPETIDAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CON LEMATIZACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from collections import Counter\n",
    "\n",
    "# # Descargar los recursos necesarios para nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # Inicializar el lematizador\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # Función para lematizar una lista de palabras\n",
    "# def lemmatize_words(words):\n",
    "#     return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# # Obtener las palabras de la columna 'title_cleaned_nostopwords'\n",
    "# # all_words = ' '.join(df_filtered['title_cleaned_nostopwords']).split()\n",
    "\n",
    "# # Lematizar las palabras\n",
    "# lemmatized_words = lemmatize_words(df_filtered['title_cleaned_nostopwords'])\n",
    "\n",
    "# # Contar la frecuencia de cada palabra lematizada\n",
    "# word_counts = Counter(lemmatized_words)\n",
    "\n",
    "# # Obtener las palabras más repetidas y mostrarlas en orden descendente\n",
    "# most_common_words = word_counts.most_common(20)\n",
    "\n",
    "# # Mostrar las palabras más repetidas\n",
    "# print('Palabras más repetidas (con lematización):')\n",
    "# for word, count in most_common_words:\n",
    "#     print(f'{word}: {count}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLO PALABRAS MÁS UTILIZADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Unir todas las palabras en una lista y eliminar los signos de puntuación\n",
    "all_words = ' '.join(df_filtered['title_cleaned_nostopwords'])\n",
    "all_words += ' '.join(df_filtered['description_nostopwords'])\n",
    "all_words += ' '.join(df_filtered['full_content_nostopwords'])\n",
    "all_words = re.findall(r'\\b\\w{2,}\\b', all_words)\n",
    "\n",
    "# Contar la frecuencia de cada palabra\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Obtener las palabras más repetidas y mostrarlas en orden descendente\n",
    "most_common_words = word_counts.most_common(20)\n",
    "\n",
    "# Mostrar las palabras más repetidas\n",
    "print('Palabras más repetidas:')\n",
    "for word, count in most_common_words:\n",
    "    print(f'{word}: {count}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORTAR BBDD CLEANED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_cleaned_nostopwords</th>\n",
       "      <th>description_nostopwords</th>\n",
       "      <th>full_content_nostopwords</th>\n",
       "      <th>victory_flag</th>\n",
       "      <th>supporters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STOP IRAN REGIME CHAIRING U​.​N . HUMAN RIGHTS...</td>\n",
       "      <td>RIGHTS COUNCIL 2023 SOCIAL FORUM . YEAR 'S THE...</td>\n",
       "      <td>JOKE : ISLAMIC REPUBLIC IRAN APPOINTED CHAIR U...</td>\n",
       "      <td>0</td>\n",
       "      <td>80515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAVE CHILDREN POVERTY SYRIA</td>\n",
       "      <td>CHILDREN SYRIA FACING EXTREME POVERTY . OFTENT...</td>\n",
       "      <td>CHILDREN SYRIA FACING EXTREME POVERTY . OFTENT...</td>\n",
       "      <td>0</td>\n",
       "      <td>23871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MOBILE OPERATORS CREATING INEQUALITY</td>\n",
       "      <td>INDIA , RELIANCE JIO ( 4G MOBILE OPERATOR ) , ...</td>\n",
       "      <td>INDIA , RELIANCE JIO ( 4G MOBILE OPERATOR ) , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRANT MILITARY BURIAL HONORS WOMEN WWII PILOTS</td>\n",
       "      <td>WORLD WAR II , BRAVE GROUP WOMEN JOINED WAR EF...</td>\n",
       "      <td>WORLD WAR II , BRAVE GROUP WOMEN JOINED WAR EF...</td>\n",
       "      <td>1</td>\n",
       "      <td>176092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>N'T VIOLATE CHARTER RIGHTS</td>\n",
       "      <td>RULINGS FIND GOVERNMENT VIOLATED FUNDAMENTAL R...</td>\n",
       "      <td>ONTARIO PREMIER DOUG FORD FOLLOWING AUGUST 201...</td>\n",
       "      <td>0</td>\n",
       "      <td>36053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>JUSTICE POUR FORTUNE</td>\n",
       "      <td>TOGOLESE STUDENT MURDERED FRANCE</td>\n",
       "      <td>TOGOLESE STUDENT MURDERED FRANCE</td>\n",
       "      <td>1</td>\n",
       "      <td>15341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>100 % CLEAN RENEWABLE ENERGY 2030 .</td>\n",
       "      <td>SWITCH 100 % CLEAN RENEWABLE ENERGY 2030 , CON...</td>\n",
       "      <td>LESS 7 YEARS EFFECTS CLIMATE CHANGE IRREVERSIB...</td>\n",
       "      <td>0</td>\n",
       "      <td>4036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>CREATE FAIR COMPASSIONATE UK IMMIGRATION POLICY</td>\n",
       "      <td>TREATED ? PETITION CHALLENGES U.K. GOVERNMENT ...</td>\n",
       "      <td>DREAM LIVE ANOTHER COUNTRY , WOULD HOPE COUNTR...</td>\n",
       "      <td>0</td>\n",
       "      <td>6393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>APPROVE SOCIAL SECURITY DISABILITY</td>\n",
       "      <td>DISABILITY DENIED EVEN THOUGH MEDICAL HISTORY ...</td>\n",
       "      <td>WANT PERSONALLY THANK EVERY ONE SUPPORT DIFFIC...</td>\n",
       "      <td>1</td>\n",
       "      <td>71938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>50:50 PARLIAMENT WANT WOMEN EQUAL SEATS EQUAL ...</td>\n",
       "      <td>. PLEASE COLLABORATE TAKE ACTION MAKE PARLIAME...</td>\n",
       "      <td>PARLIAMENT MEN OUTNUMBER WOMEN 2:1 . 12 EXTRA ...</td>\n",
       "      <td>0</td>\n",
       "      <td>53865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1658 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title_cleaned_nostopwords  \\\n",
       "0     STOP IRAN REGIME CHAIRING U​.​N . HUMAN RIGHTS...   \n",
       "1                           SAVE CHILDREN POVERTY SYRIA   \n",
       "3                  MOBILE OPERATORS CREATING INEQUALITY   \n",
       "4        GRANT MILITARY BURIAL HONORS WOMEN WWII PILOTS   \n",
       "5                            N'T VIOLATE CHARTER RIGHTS   \n",
       "...                                                 ...   \n",
       "1960                               JUSTICE POUR FORTUNE   \n",
       "1961                100 % CLEAN RENEWABLE ENERGY 2030 .   \n",
       "1962    CREATE FAIR COMPASSIONATE UK IMMIGRATION POLICY   \n",
       "1963                 APPROVE SOCIAL SECURITY DISABILITY   \n",
       "1964  50:50 PARLIAMENT WANT WOMEN EQUAL SEATS EQUAL ...   \n",
       "\n",
       "                                description_nostopwords  \\\n",
       "0     RIGHTS COUNCIL 2023 SOCIAL FORUM . YEAR 'S THE...   \n",
       "1     CHILDREN SYRIA FACING EXTREME POVERTY . OFTENT...   \n",
       "3     INDIA , RELIANCE JIO ( 4G MOBILE OPERATOR ) , ...   \n",
       "4     WORLD WAR II , BRAVE GROUP WOMEN JOINED WAR EF...   \n",
       "5     RULINGS FIND GOVERNMENT VIOLATED FUNDAMENTAL R...   \n",
       "...                                                 ...   \n",
       "1960                   TOGOLESE STUDENT MURDERED FRANCE   \n",
       "1961  SWITCH 100 % CLEAN RENEWABLE ENERGY 2030 , CON...   \n",
       "1962  TREATED ? PETITION CHALLENGES U.K. GOVERNMENT ...   \n",
       "1963  DISABILITY DENIED EVEN THOUGH MEDICAL HISTORY ...   \n",
       "1964  . PLEASE COLLABORATE TAKE ACTION MAKE PARLIAME...   \n",
       "\n",
       "                               full_content_nostopwords  victory_flag  \\\n",
       "0     JOKE : ISLAMIC REPUBLIC IRAN APPOINTED CHAIR U...             0   \n",
       "1     CHILDREN SYRIA FACING EXTREME POVERTY . OFTENT...             0   \n",
       "3     INDIA , RELIANCE JIO ( 4G MOBILE OPERATOR ) , ...             0   \n",
       "4     WORLD WAR II , BRAVE GROUP WOMEN JOINED WAR EF...             1   \n",
       "5     ONTARIO PREMIER DOUG FORD FOLLOWING AUGUST 201...             0   \n",
       "...                                                 ...           ...   \n",
       "1960                   TOGOLESE STUDENT MURDERED FRANCE             1   \n",
       "1961  LESS 7 YEARS EFFECTS CLIMATE CHANGE IRREVERSIB...             0   \n",
       "1962  DREAM LIVE ANOTHER COUNTRY , WOULD HOPE COUNTR...             0   \n",
       "1963  WANT PERSONALLY THANK EVERY ONE SUPPORT DIFFIC...             1   \n",
       "1964  PARLIAMENT MEN OUTNUMBER WOMEN 2:1 . 12 EXTRA ...             0   \n",
       "\n",
       "      supporters  \n",
       "0          80515  \n",
       "1          23871  \n",
       "3            350  \n",
       "4         176092  \n",
       "5          36053  \n",
       "...          ...  \n",
       "1960       15341  \n",
       "1961        4036  \n",
       "1962        6393  \n",
       "1963       71938  \n",
       "1964       53865  \n",
       "\n",
       "[1658 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear una nueva DataFrame con las columnas específicas\n",
    "df_clean_train = df_filtered[['title_cleaned_nostopwords', 'description_nostopwords', 'full_content_nostopwords', 'victory_flag', 'supporters']].copy()\n",
    "df_clean_train.to_csv('dataframe_train.csv', index=False)\n",
    "df_clean_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOTEAR IMAGEN CON PALABRAS MAS REPETIDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Crear una wordcloud con las palabras más repetidas\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(most_common_words))\n",
    "\n",
    "# Mostrar la wordcloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARA MODELO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature  Importance\n",
      "9183   company    0.878150\n",
      "39512  surgery    0.877249\n",
      "486       2013    0.867892\n",
      "41083     trai    0.846458\n",
      "25611  matthew    0.817679\n",
      "19431     hens    0.816710\n",
      "14065     eggs    0.752160\n",
      "7067      cage    0.730241\n",
      "43069  victims    0.720397\n",
      "8467     cispa    0.681496\n",
      "24198     like    0.665872\n",
      "32950      put    0.649694\n",
      "17145   france    0.634275\n",
      "17739      gay    0.632598\n",
      "33399     rape    0.629750\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       285\n",
      "           1       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.86       332\n",
      "   macro avg       0.43      0.50      0.46       332\n",
      "weighted avg       0.74      0.86      0.79       332\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuzu9627/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zuzu9627/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zuzu9627/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# ATRIBUTOS MÁS IMPORTANTES\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Crear una matriz TF-IDF a partir de los campos de texto relevantes\n",
    "tfidf = TfidfVectorizer()\n",
    "text_features = tfidf.fit_transform(df_clean_train['full_content_nostopwords'])\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_features, df_clean_train['victory_flag'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar un modelo de regresión logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "feature_importances = model.coef_[0]\n",
    "\n",
    "# Crear un DataFrame con las características y sus importancias\n",
    "feature_df = pd.DataFrame({'Feature': tfidf.get_feature_names_out(), 'Importance': feature_importances})\n",
    "feature_df = feature_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Imprimir las 10 características más importantes\n",
    "top_features = feature_df.head(15)\n",
    "print(top_features)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTRO MODELO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para utilizar las palabras más repetidas como atributos importantes en un modelo de predicción, puedes seguir los siguientes pasos:\n",
    "Obtener las palabras más repetidas del DataFrame 'df_filtered' en la columna 'title_cleaned_nostopwords' utilizando el script que hemos modificado anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Obtener solo las palabras más repetidas (sin contar la frecuencia)\n",
    "top_words = [word for word, _ in most_common_words]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizar las palabras más repetidas como atributos importantes en el modelo de predicción. En lugar de utilizar el objeto TfidfVectorizer para extraer características de todo el texto, utilizaremos solo las palabras más repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92       285\n",
      "           1       0.00      0.00      0.00        47\n",
      "\n",
      "    accuracy                           0.86       332\n",
      "   macro avg       0.43      0.50      0.46       332\n",
      "weighted avg       0.74      0.86      0.79       332\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuzu9627/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zuzu9627/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zuzu9627/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Crear un vectorizador que solo incluya las palabras más repetidas\n",
    "vectorizer = CountVectorizer(vocabulary=top_words)\n",
    "\n",
    "# Obtener las características utilizando el vectorizador\n",
    "text_features = vectorizer.transform(df_filtered['title_cleaned_nostopwords'])\n",
    "text_features += vectorizer.transform(df_filtered['description_nostopwords'])\n",
    "text_features += vectorizer.transform(df_filtered['full_content_nostopwords'])\n",
    "\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_features, df_filtered['victory_flag'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar un modelo de regresión logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
